import numpy as np
from enum import Enum
import copy

Consider a standard grid world, where only 4 (up, down, left, right) actions are allowed and the agent deterministically moves accordingly, represented as below. Here yellow is the start state and white is the goal state.

Say, we define our MDP as:
- S: 121 (11 x 11) cells 
- A: 4 actions (up, down, left, right)
- P: Deterministic transition probability
- R: -1 at every step
- gamma: 0.9

Our goal is to find an optimal policy (shown in right).



![image.png](data:image/png;base64,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)

# Above grid is defined as below:
#   - 0 denotes an navigable tile
#   - 1 denotes an obstruction/wall
#   - 2 denotes the start state 
#   - 3 denotes an goal state

# Note: Here the upper left corner is defined as (0, 0)
#       and lower right corner as (m-1, n-1)

# Optimal Path: RIGHT RIGHT UP UP LEFT LEFT UP UP UP UP UP UP LEFT LEFT DOWN DOWN LEFT LEFT


GRID_WORLD = np.array([
    [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],
    [1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1],
    [1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1],
    [1, 3, 0, 0, 1, 0, 1, 0, 1, 0, 1],
    [1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1],
    [1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1],
    [1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1],
    [1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1],
    [1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1],
    [1, 0, 0, 0, 0, 2, 0, 0, 0, 0, 1],
    [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]
])

### Actions

class Actions(Enum):
  UP    = (0, (-1, 0))  # index = 0, (xaxis_move = -1 and yaxis_move = 0)
  DOWN  = (1, (1, 0))   # index = 1, (xaxis_move = 1 and yaxis_move = 0) 
  LEFT  = (2, (0, -1))  # index = 2, (xaxis_move = 0 and yaxis_move = -1)
  RIGHT = (3, (0, 1))   # index = 3, (xaxis_move = 0 and yaxis_move = -1)

  def get_action_dir(self):
    _, direction = self.value
    return direction

  @property
  def index(self):
    indx, _ = self.value
    return indx

  @classmethod
  def from_index(cls, index):
    action_index_map = {a.index: a for a in cls}
    return action_index_map[index]

# How to use Action enum
for a in Actions:
  print(f"name: {a.name}, action_id: {a.index}, direction_to_move: {a.get_action_dir()}")

print("\n------------------------------------\n")

# find action enum from index 0
a = Actions.from_index(0)
print(f"0 index action is: {a.name}")

### Policy

class BasePolicy:
  def update(self, *args):
    pass

  def select_action(self, state_id: int) -> int:
    raise NotImplemented


class DeterministicPolicy(BasePolicy):
  def __init__(self, actions: np.ndarray):
    # actions: its a 1d array (|S| size) which contains action for each state
    self.actions = actions

  def update(self, state_id, action_id):
    assert state_id < len(self.actions), f"Invalid state_id {state_id}"
    assert action_id < len(Actions), f"Invalid action_id {action_id}"
    self.actions[state_id] = action_id

  def select_action(self, state_id: int) -> int:
    assert state_id < len(self.actions), f"Invalid state_id {state_id}"
    return self.actions[state_id]

### Environment

class Environment:
  def __init__(self, grid):
    self.grid = grid
    m, n = grid.shape
    self.num_states = m*n

  def xy_to_posid(self, x: int, y: int):
    _, n = self.grid.shape
    return x*n + y

  def posid_to_xy(self, posid: int):
    _, n = self.grid.shape
    return (posid // n, posid % n)

  def isvalid_move(self, x: int, y: int):
    m, n = self.grid.shape
    return (x >= 0) and (y >= 0) and (x < m) and (y < n) and (self.grid[x, y] != 1)

  def find_start_xy(self) -> int:
    m, n = self.grid.shape
    for x in range(m):
      for y in range(n):
        if self.grid[x, y] == 2:
          return (x, y)
    raise Exception("Start position not found.")

  def find_path(self, policy: BasePolicy) -> str:
    max_steps = 50
    steps = 0

    P, R = self.get_transition_prob_and_expected_reward()
    num_actions, num_states = R.shape
    all_possible_state_posids = np.arange(num_states)

    path = ""
    curr_x, curr_y = self.find_start_xy()
    while (self.grid[curr_x, curr_y] != 3) and (steps < max_steps):
      curr_posid = self.xy_to_posid(curr_x, curr_y)
      action_id = policy.select_action(curr_posid)
      next_posid = np.random.choice(
          all_possible_state_posids, p=P[action_id, curr_posid])
      action = Actions.from_index(action_id)
      path += f" {action.name}"
      curr_x, curr_y = self.posid_to_xy(next_posid)
      steps += 1
    return path

  def get_transition_prob_and_expected_reward(self):  # P(s_next | s, a), R(s, a)
    m, n = self.grid.shape
    num_states = m*n
    num_actions = len(Actions)
    P = np.zeros((num_actions, num_states, num_states))
    R = np.zeros((num_actions, num_states))
    for a in Actions:
      for x in range(m):
        for y in range(n):
          xmove_dir, ymove_dir = a.get_action_dir()
          xnew, ynew = x + xmove_dir, y + ymove_dir  # find the new co-ordinate after the action a

          posid = self.xy_to_posid(x, y)
          new_posid = self.xy_to_posid(xnew, ynew)
  
          
          if self.grid[x, y] == 3:
            # the current state is a goal state
            P[a.index, posid, posid] = 1
            R[a.index, posid] = 0
          elif (self.grid[x, y] == 1) or (not self.isvalid_move(xnew, ynew)):
            # the current state is a block state or the next state is invalid
            P[a.index, posid, posid] = 1
            R[a.index, posid] = -1
          else:
            # action a is valid and goes to a new position
            P[a.index, posid, new_posid] = 1
            R[a.index, posid] = -1
    return P, R

### Policy Iteration

def policy_evaluation(P: np.ndarray, R: np.ndarray, gamma: float,
                      policy: BasePolicy, theta: float,
                      init_V: np.ndarray=None):
  num_actions, num_states = R.shape

  # Please try different starting point for V you will find it will always
  # converge to the same V_pi value.
  if init_V is None:
    init_V = np.zeros(num_states)
  V = copy.deepcopy(init_V)

  delta = 100.0
  while delta > theta:
    delta = 0.0
    for state_id in range(num_states):
      action_id = policy.select_action(state_id)
      v_old = V[state_id]
      # Following equation is a different way of writing the same equation given in the slide.
      # Note here R is an expected reward term.
      V[state_id] = R[action_id, state_id] + gamma * np.dot(P[action_id, state_id], V)
      delta = max(delta, abs(V[state_id] - v_old))
  return V


def policy_improvement(P: np.ndarray, R: np.ndarray, gamma: float,
                       policy: BasePolicy, V: np.ndarray):
  num_actions, num_states = R.shape
  policy_stable = True
  for state_id in range(num_states):
    old_action_id = policy.select_action(state_id)

    # Update new_action_id based on the value function.
    best_action_id = 0
    best_action_value = -float('inf')
    for action_id in range(num_actions):
      action_value = R[action_id, state_id] + gamma * np.dot(P[action_id, state_id], V)
      if action_value > best_action_value:
        best_action_value = action_value
        best_action_id = action_id
    new_action_id = best_action_id
    
    policy.update(state_id, new_action_id)
    if old_action_id != new_action_id:
      policy_stable = False
  return policy_stable


def policy_iteration(P: np.ndarray, R: np.ndarray, gamma: float,
                     theta: float=1e-3, init_policy: BasePolicy = None):
  num_actions, num_states = R.shape

  # Please try exploring different policies you will find it will always
  # converge to the same optimal policy for valid states.
  if init_policy is None:
    # Say initial policy = all up actions.
    init_policy = DeterministicPolicy(actions=np.zeros(num_states, dtype=int))

  # creating a copy of a initial policy
  policy = copy.deepcopy(init_policy)
  policy_stable = False
  while not policy_stable:
    V = policy_evaluation(P, R, gamma, policy, theta)
    policy_stable = policy_improvement(P, R, gamma, policy, V)
  return policy, V

### Value Iteration

# Directly find the optimal value function
def get_optimal_value(P: np.ndarray, R: np.ndarray, gamma: float,
                      theta: float, init_V: np.ndarray=None):
  num_actions, num_states = R.shape

  # Please try different starting point for V you will find it will always
  # converge to the same V_star value.
  if init_V is None:
    init_V = np.zeros(num_states)
  V = copy.deepcopy(init_V)

  delta = 100.0
  while delta > theta:
    delta = 0.0
    for state_id in range(num_states):
      v_old = V[state_id]
      q_sa = np.zeros(num_actions)
      for a in Actions:
        q_sa[a.index] = R[a.index, state_id] + gamma * np.dot(P[a.index, state_id], V)
      V[state_id] = np.max(q_sa)
      delta = max(delta, abs(V[state_id] - v_old))
  return V


def value_iteration(P: np.ndarray, R: np.ndarray, gamma: float,
                    theta: float=1e-3, init_V: np.ndarray=None):
  V_star = get_optimal_value(P, R, gamma, theta, init_V)

  num_actions, num_states = R.shape
  policy = DeterministicPolicy(actions=np.zeros(num_states, dtype=int))
  for state_id in range(num_states):
    best_action_id = 0
    best_action_value = -float('inf')
    for action_id in range(num_actions):
      action_value = R[action_id, state_id] + gamma * np.dot(P[action_id, state_id], V_star)
      if action_value > best_action_value:
        best_action_value = action_value
        best_action_id = action_id

    policy.update(state_id, best_action_id)
  
  return policy, V_star

### Experiments

def is_same_optimal_value(V1, V2, diff_theta=1e-3):
  diff = np.abs(V1 - V2)
  return np.all(diff < diff_theta)

seed = 0
np.random.seed(seed)

gamma = 0.9
theta = 1e-5

env = Environment(GRID_WORLD)
P, R = env.get_transition_prob_and_expected_reward()

#### Exp 1: Using Policy iteration algorithm find the optimal path from start to goal position

# # Start with random choice of init_policy.
# One such choice could be: init_policy = np.ones(env.num_states, dtype=int)
# Start with your own choice of init_policy
init_policy = DeterministicPolicy(actions=np.ones(env.num_states, dtype=int))

pitr_policy, pitr_V_star = policy_iteration(P, R, gamma, theta=theta, init_policy=init_policy)
pitr_path = env.find_path(pitr_policy)
print(pitr_path)

#### Exp 2: Using value iteration algorithm find the optimal path from start to goal position

vitr_policy, vitr_V_star = value_iteration(P, R, gamma, theta=theta)
vitr_path = env.find_path(vitr_policy)
print(vitr_path)

#### Exp 3: Compare the optimal value function of policy iteration and value iteration algorithm

is_same_optimal_value(pitr_V_star, vitr_V_star)

#### Exp 4: Using initial guess for V as random values, find the optimal value function using policy evaluation and compare it with the optimal value function

# Start with random choice of init_V.
# One such choice could be: init_V = np.random.randn(env.num_states)
# Another choice could be: init_V = 10*np.ones(env.num_states)
# Start with your own choice of init_V
init_V = 10*np.ones(env.num_states) # your choice

V_star = policy_evaluation(P, R, gamma, pitr_policy, theta, init_V)
is_same_optimal_value(pitr_V_star, V_star)

#### Exp 5: Using initial guess for V as random values, find the optimal value function using get_optimal_value and compare it with the optimal value function

# Start with random choice.
# One such choice could be: init_V = np.random.randn(env.num_states)
# Another choice could be: init_V = 10*np.ones(env.num_states)
# Start with your own choice of init_V
init_V = 100*np.ones(env.num_states)

V_star = get_optimal_value(P, R, gamma, theta, init_V)
is_same_optimal_value(vitr_V_star, V_star)

#### Exp Optional: Try changing the grid by adding multiple paths to the goal state and check if our policy_iteration or value_iteration algorithm is able to find optimal path. Redo the above experiments. 



GRID_WORLD[4, 1] = 0

GRID_WORLD = np.array([
    [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],
    [1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1],
    [1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1],
    [1, 3, 0, 0, 1, 0, 1, 0, 1, 0, 1],
    [1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1],
    [1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1],
    [1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1],
    [1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1],
    [1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1],
    [1, 0, 0, 0, 0, 2, 0, 0, 0, 0, 1],
    [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]
])

env = Environment(GRID_WORLD)
P, R = env.get_transition_prob_and_expected_reward()

init_policy = DeterministicPolicy(actions=np.ones(env.num_states, dtype=int))

pitr_policy, pitr_V_star = policy_iteration(P, R, gamma, theta=theta, init_policy=init_policy)
pitr_path = env.find_path(pitr_policy)
print(pitr_path)

vitr_policy, vitr_V_star = value_iteration(P, R, gamma, theta=theta)
vitr_path = env.find_path(vitr_policy)
print(vitr_path)


is_same_optimal_value(pitr_V_star, vitr_V_star)

init_V = 10*np.ones(env.num_states) # your choice

V_star = policy_evaluation(P, R, gamma, pitr_policy, theta, init_V)
is_same_optimal_value(pitr_V_star, V_star)

init_V = 100*np.ones(env.num_states)

V_star = get_optimal_value(P, R, gamma, theta, init_V)
is_same_optimal_value(vitr_V_star, V_star)

GRID_WORLD[3,6]=0

GRID_WORLD = np.array([
    [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],
    [1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1],
    [1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1],
    [1, 3, 0, 0, 1, 0, 1, 0, 1, 0, 1],
    [1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1],
    [1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1],
    [1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1],
    [1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1],
    [1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1],
    [1, 0, 0, 0, 0, 2, 0, 0, 0, 0, 1],
    [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]
])


env = Environment(GRID_WORLD)
P, R = env.get_transition_prob_and_expected_reward()

init_policy = DeterministicPolicy(actions=np.ones(env.num_states, dtype=int))

pitr_policy, pitr_V_star = policy_iteration(P, R, gamma, theta=theta, init_policy=init_policy)
pitr_path = env.find_path(pitr_policy)
print(pitr_path)

vitr_policy, vitr_V_star = value_iteration(P, R, gamma, theta=theta)
vitr_path = env.find_path(vitr_policy)
print(vitr_path)

is_same_optimal_value(pitr_V_star, vitr_V_star)

init_V = 10*np.ones(env.num_states) # your choice

V_star = policy_evaluation(P, R, gamma, pitr_policy, theta, init_V)
is_same_optimal_value(pitr_V_star, V_star)

init_V = 100*np.ones(env.num_states)

V_star = get_optimal_value(P, R, gamma, theta, init_V)
is_same_optimal_value(vitr_V_star, V_star)
