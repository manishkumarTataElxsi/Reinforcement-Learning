import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
import pandas as pd
from IPython.display import display, HTML
from typing import NamedTuple, List

### Gaussian Bandit Environment

class GaussianArm(NamedTuple):
  mean: float
  std: float


class Env:
  def __init__(self, num_arms: int, mean_reward_range: tuple, std: float):
    """
    num_arms: number of bandit arms
    mean_reward_range: mean reward of an arm should lie between the given range
    std: standard deviation of the reward for each arm
    """
    self.num_arms = num_arms
    self.arms = self.create_arms(num_arms, mean_reward_range, std)

  def create_arms(self, n: int, mean_reward_range: tuple, std: float) -> dict:
    low_rwd, high_rwd = mean_reward_range
    # creates "n" number of mean reward for each arm
    means = np.random.uniform(low=low_rwd, high=high_rwd, size=(n,))
    arms = {id: GaussianArm(mu, std) for id, mu in enumerate(means)}
    return arms

  @property
  def arm_ids(self):
    return list(self.arms.keys())

  def step(self, arm_id: int) -> float:
    arm = self.arms[arm_id]
    return np.random.normal(arm.mean, arm.std)   # Reward

  def get_best_arm_and_expected_reward(self):
    best_arm_id = max(self.arms, key=lambda x: self.arms[x].mean)
    return best_arm_id, self.arms[best_arm_id].mean

  def get_avg_arm_reward(self):
    arm_mean_rewards = [v.mean for v in self.arms.values()]
    return np.mean(arm_mean_rewards)

  def plot_arms_reward_distribution(self, num_samples=1000):
    """
    This function is only used to visualize the arm's distrbution.
    """
    fig, ax = plt.subplots(1, 1, sharex=False, sharey=False, figsize=(9, 5))
    colors = sns.color_palette("hls", self.num_arms)
    for i, arm_id in enumerate(self.arm_ids):
      reward_samples = [self.step(arm_id) for _ in range(num_samples)]
      sns.histplot(reward_samples, ax=ax, stat="density", kde=True, bins=100, color=colors[i], label=f'arm_{arm_id}')
    ax.legend()
    plt.show()

### Policy

class BasePolicy:
  @property
  def name(self):
    return 'base_policy'

  def reset(self):
    """
    This function resets the internal variable.
    """
    pass

  def update_arm(self, *args):
    """
    This function keep track of the estimates
    that we may want to update during training.
    """
    pass

  def select_arm(self) -> int:
    """
    It returns arm_id
    """
    raise Exception("Not Implemented")

#### Random Policy

class RandomPolicy(BasePolicy):
  def __init__(self, arm_ids: List[int]):
    self.arm_ids = arm_ids

  @property
  def name(self):
    return 'random'

  def reset(self) -> None:
    """No use."""
    pass

  def update_arm(self, *args) -> None:
    """No use."""
    pass

  def select_arm(self) -> int:
    return np.random.choice(self.arm_ids)

class RandomPolicy:
    def __init__(self, num_arms):
        self.num_arms = num_arms

    def select_arm(self):
        return np.random.choice(self.num_arms)

###Epsilon Greedy Policy

class EpGreedyPolicy(BasePolicy):
  def __init__(self, epsilon: float, arm_ids: List[int]):
    self.epsilon = epsilon
    self.arm_ids = arm_ids
    self.Q = {id: 0 for id in self.arm_ids}
    self.num_pulls_per_arm = {id: 0 for id in self.arm_ids}

  @property
  def name(self):
    return f'ep-greedy ep:{self.epsilon}'

  def reset(self) -> None:
    self.Q = {id: 0 for id in self.arm_ids}
    self.num_pulls_per_arm = {id: 0 for id in self.arm_ids}

  def update_arm(self, arm_id: int, arm_reward: float) -> None:
    # your code for updating the Q values of each arm
    self.num_pulls_per_arm[arm_id] += 1
    self.Q[arm_id] += (arm_reward - self.Q[arm_id]) / self.num_pulls_per_arm[arm_id]

  def select_arm(self) -> int:
    #code for selecting arm based on epsilon greedy policy
    if np.random.rand() < self.epsilon:
        return np.random.choice(self.arm_ids)
    return max(self.Q, key=self.Q.get)


###Softmax Policy

class SoftmaxPolicy(BasePolicy):
  def __init__(self, tau, arm_ids):
    self.tau = tau
    self.arm_ids = arm_ids
    self.Q = {id: 0 for id in self.arm_ids}
    self.num_pulls_per_arm = {id: 0 for id in self.arm_ids}

  @property
  def name(self):
    return f'softmax tau:{self.tau}'

  def reset(self):
    self.Q = {id: 0 for id in self.arm_ids}
    self.num_pulls_per_arm = {id: 0 for id in self.arm_ids}

  def update_arm(self, arm_id: int, arm_reward: float) -> None:
    # code for updating the Q values of each arm
        self.num_pulls_per_arm[arm_id] += 1
        self.Q[arm_id] += (arm_reward - self.Q[arm_id]) / self.num_pulls_per_arm[arm_id]

  def select_arm(self) -> int:
      # code for selecting arm based on softmax policy
      Q_values = np.array(list(self.Q.values()))
      # we have to normalize to prevent overflow
      Q_values -= np.max(Q_values)  # subtracting max Q-value to stabilize exp()
      exp_Q = np.exp(Q_values / self.tau)
      probs = exp_Q / np.sum(exp_Q)

      return np.random.choice(self.arm_ids, p=probs)



class UCB(BasePolicy):
    def __init__(self, c: float, arm_ids: list):
        self.c = c  #exploration parameter
        self.arm_ids = arm_ids
        self.Q = {id: 0 for id in self.arm_ids}
        self.num_pulls_per_arm = {id: 0 for id in self.arm_ids}
        self.total_pulls = 0

    @property
    def name(self):
        return f'UCB c:{self.c}'

    def reset(self):
        self.Q = {id: 0 for id in self.arm_ids}
        self.num_pulls_per_arm = {id: 0 for id in self.arm_ids}
        self.total_pulls = 0

    def update_arm(self, arm_id: int, arm_reward: float):
        self.num_pulls_per_arm[arm_id] += 1
        self.total_pulls += 1
        self.Q[arm_id] += (arm_reward - self.Q[arm_id]) / self.num_pulls_per_arm[arm_id]

    def select_arm(self) -> int:
        for arm_id in self.arm_ids:
            if self.num_pulls_per_arm[arm_id] == 0:
                return arm_id

        ucb_values = {arm_id: self.Q[arm_id] + self.c * np.sqrt(np.log(self.total_pulls) / self.num_pulls_per_arm[arm_id])
                      for arm_id in self.arm_ids}
        return max(ucb_values, key=ucb_values.get)

#### Trainer

def train(env, policy: BasePolicy, timesteps):
  policy_reward = np.zeros((timesteps,))
  for t in range(timesteps):
    arm_id = policy.select_arm()
    reward = env.step(arm_id)
    policy.update_arm(arm_id, reward)
    policy_reward[t] = reward
  return policy_reward


def avg_over_runs(env, policy: BasePolicy, timesteps, num_runs):
    _, expected_max_reward = env.get_best_arm_and_expected_reward()
    policy_reward_each_run = np.zeros((num_runs, timesteps))

    for run in range(num_runs):
        policy.reset()
        policy_reward = train(env, policy, timesteps)
        policy_reward_each_run[run, :] = policy_reward

    # calculate avg policy reward from policy_reward_each_run
    avg_policy_rewards = np.mean(policy_reward_each_run, axis=0)
    total_policy_regret = np.sum(expected_max_reward - avg_policy_rewards)

    return avg_policy_rewards, total_policy_regret

Plot Reward and Regret Curve

def plot_reward_curve_and_print_regret(env, policies, timesteps=200, num_runs=500):
  fig, ax = plt.subplots(1, 1, sharex=False, sharey=False, figsize=(10, 6))
  for policy in policies:
    avg_policy_rewards, total_policy_regret = avg_over_runs(env, policy, timesteps, num_runs)
    print('regret for {}: {:.3f}'.format(policy.name, total_policy_regret))
    ax.plot(np.arange(timesteps), avg_policy_rewards, '-', label=policy.name)

  _, expected_max_reward = env.get_best_arm_and_expected_reward()
  ax.plot(np.arange(timesteps), [expected_max_reward]*timesteps, 'g-')

  avg_arm_reward = env.get_avg_arm_reward()
  ax.plot(np.arange(timesteps), [avg_arm_reward]*timesteps, 'r-')

  plt.legend(loc='lower right')
  plt.show()

### Experiments

seed = 42
np.random.seed(seed)

num_arms = 5
mean_reward_range = (-25, 25)
std = 2.0

env = Env(num_arms, mean_reward_range, std)

env.plot_arms_reward_distribution()

best_arm, max_mean_reward = env.get_best_arm_and_expected_reward()
print(best_arm, max_mean_reward)

print(env.get_avg_arm_reward())

random_policy = RandomPolicy(env.arm_ids)

plot_reward_curve_and_print_regret(env, [random_policy], timesteps=500, num_runs=500)


#### Please explore following values:

- Epsilon greedy: [0.001, 0.01, 0.5, 0.9]
- Softmax: [0.001, 1.0, 5.0, 50.0]

### Analysis of ε-Greedy Policy Regret Across Different Epsiolon at different Timesteps

explore_epgreedy_epsilons =  [0.001, 0.01, 0.5, 0.9]
epgreedy_policies = [EpGreedyPolicy(ep, env.arm_ids) for ep in explore_epgreedy_epsilons]

#experimenting with different timesteps , timesteps represents the number of trials the policy has with the environment
print("timestep:5")
plot_reward_curve_and_print_regret(env, epgreedy_policies, timesteps=5, num_runs=500)
print("timestep:10")
plot_reward_curve_and_print_regret(env, epgreedy_policies, timesteps=10, num_runs=500)
print("timestep:20")
plot_reward_curve_and_print_regret(env, epgreedy_policies, timesteps=20, num_runs=500)
print("timestep:50, num_runs:500")
plot_reward_curve_and_print_regret(env, epgreedy_policies, timesteps=50, num_runs=500)
print("timestep:100, num_runs:500")
plot_reward_curve_and_print_regret(env, epgreedy_policies, timesteps=100, num_runs=500)
print("timestep:200, num_runs:500")
plot_reward_curve_and_print_regret(env, epgreedy_policies, timesteps=200, num_runs=500)
print("timestep:500, num_runs:500")
plot_reward_curve_and_print_regret(env, epgreedy_policies, timesteps=500, num_runs=500)
print("timestep:1000, num_runs:500")
plot_reward_curve_and_print_regret(env, epgreedy_policies, timesteps=1000, num_runs=500)
print("timestep:10000, num_runs:500")
plot_reward_curve_and_print_regret(env, epgreedy_policies, timesteps=10000, num_runs=500)
#for small timesteps,Policies may not have enough trials to differentiate between good and bad arms.
#for medium timesteps, Policies start converging to the best arm.
#for large timesteps, Policies should have fully learned the best arm.

Hence,
The regret values for different values of ε (epsilon) in the ε-greedy policy show exploration vs. exploitation.
Low Epsilon 0.001, 0.01, leads Lower Regret,
At smaller timesteps 5-50, both ε=0.001 and ε=0.01 have the lowest regret.
This is because these policies explore less and exploit more, quickly identifying high-reward arms.

Higher Epsilon 0.5, 0.9 leads, higher Regret,
ε=0.5 and ε=0.9 incur drastically higher regret as they explore too much and fail to settle on optimal arms early.
For example, at timestep 50, regret for ε=0.9,873.234 is more than 20 times that of ε=0.001 28.948.

As timesteps increase, ε=0.5 and ε=0.9 show exponential regret growth (e.g., at timestep 10,000, regret reaches 174,754 for ε=0.9).
This suggests that excessive exploration prevents convergence to optimal arms.

Small epsilon eventually leads to higher regret,Initially, ε=0.001 has the lowest regret, but over time e.g., at timestep 10,000, regret = 512.059, it falls behind ε=0.01,1982.046.
This suggests that some exploration is necessary to avoid local optima.

Best Epsilon Choice,for short-term optimality (small timesteps), use ε=0.001.
For long-term performance, ε=0.01 balances exploration and exploitation well.
Avoid ε=0.5 and ε=0.9 unless exploration is a priority.

### Analysis of Softmax Policy Regret Across Different Timesteps and Temperature (τ) Values

explore_softmax_taus =  [0.001, 1.0, 5.0, 50.0]
softmax_polices = [SoftmaxPolicy(tau, env.arm_ids) for tau in explore_softmax_taus]
print("Timestep:5")
plot_reward_curve_and_print_regret(env, softmax_polices, timesteps=50, num_runs=500)
print("Timestep:10")
plot_reward_curve_and_print_regret(env, softmax_polices, timesteps=50, num_runs=500)
print("Timestep:20")
plot_reward_curve_and_print_regret(env, softmax_polices, timesteps=50, num_runs=500)
print("Timestep:50")
plot_reward_curve_and_print_regret(env, softmax_polices, timesteps=50, num_runs=500)
print("Timestep:100")
plot_reward_curve_and_print_regret(env, softmax_polices, timesteps=100, num_runs=500)
print("Timestep:200")
plot_reward_curve_and_print_regret(env, softmax_polices, timesteps=200, num_runs=500)
print("Timestep:500")
plot_reward_curve_and_print_regret(env, softmax_polices, timesteps=500, num_runs=500)
print("Timestep:1000")
plot_reward_curve_and_print_regret(env, softmax_polices, timesteps=1000, num_runs=500)
print("Timestep:10000")
plot_reward_curve_and_print_regret(env, softmax_polices, timesteps=10000, num_runs=500)

The Softmax exploration strategy uses a temperature parameter (τ) to balance exploration and exploitation. Higher τ values encourage more exploration, while lower τ values favor exploitation.

Low Tau (0.001) = Highest Regret

Across all timesteps, τ=0.001 consistently has the highest regret (e.g., 476.925 at timestep 5 and 95,270 at timestep 10,000).
This is because low τ results in near-deterministic behavior, exploiting too early without adequate exploration.
Moderate Tau (5.0) = Lowest Regret

τ=5.0 achieves the lowest regret in early and long-term performance (e.g., 199.032 at timestep 5, 910.267 at timestep 500, and 16,207 at timestep 10,000).
This suggests that τ=5.0 provides a good balance between exploration and exploitation.

High Tau (50.0) = Very High Regret

τ=50.0 performs the worst in most cases (e.g., 807.466 at timestep 5, 15,730 at timestep 1,000).
This happens because excessive randomness causes too much exploration without converging to the best arms.
τ=0.001 and τ=50.0 show exponential regret growth, meaning too little or too much randomness is detrimental.
τ=5.0 scales the best, achieving the lowest regret consistently across time.
Short vs.

In the short term (timestep ≤ 50), regret is still relatively low for all values.
Beyond 500 timesteps, τ=5.0 performs best, while τ=0.001 and τ=50.0 diverge sharply.

Hence,best Tau choice
For optimal performance, τ=5.0 is the best balance between exploration and exploitation.



### Analysis of Upper Confidence Bound (UCB) Regret Across Timesteps and Exploration Parameters (c)

explore_ucb_c = [0.05, 0.1, 0.5, 1.0, 1.5, 2.0]
ubc_policies = [UCB(c, env.arm_ids) for c in explore_ucb_c]
print("Timestep:5")
plot_reward_curve_and_print_regret(env, ubc_policies, timesteps=5, num_runs=500)
print("Timestep:10")
plot_reward_curve_and_print_regret(env, ubc_policies, timesteps=10, num_runs=500)
print("Timestep:50")
plot_reward_curve_and_print_regret(env, ubc_policies, timesteps=50, num_runs=500)
print("Timestep:100")
plot_reward_curve_and_print_regret(env, ubc_policies, timesteps=100, num_runs=500)
print("Timestep:200")
plot_reward_curve_and_print_regret(env, ubc_policies, timesteps=200, num_runs=500)
print("Timestep:500")
plot_reward_curve_and_print_regret(env, ubc_policies, timesteps=500, num_runs=500)

The Upper Confidence Bound (UCB) algorithm balances exploration and exploitation by using an uncertainty term controlled by c. A higher c encourages more exploration, while a lower c focuses on exploitation.


Unlike Softmax or Epsilon-Greedy, UCB exhibits very low regret values (≈ 95-100) across all timesteps.
This suggests that UCB consistently balances exploration and exploitation well.
Minimal Variation Across Different c Values

The regret values for different c values are very close (e.g., at timestep 100, values range between 95.755 and 98.181).
This indicates that UCB is relatively insensitive to c within the range [0.05, 2.0].
Small c (0.05) and Large c (2.0) Have Comparable Regret

At timestep 500, regret for c=0.05 is 92.122, and for c=2.0 is 93.656, showing that both small and large c values perform similarly.
Slight Regret Increase for Mid-Range c Values at Larger Timesteps

At timestep 200, c=0.5 and c=1.5 have the highest regret (99.323 and 98.415, respectively).
This suggests that moderate values of c might slightly over-explore compared to very small or large values.
Unlike Softmax or Epsilon-Greedy, UCB Regret Remains Bounded

Softmax and Epsilon-Greedy showed exponential regret growth over time, but UCB remains stable (≈ 95-100).
This highlights the superior theoretical guarantees of UCB in regret minimization.
Hence,best c Value,
For robustness, c=0.1 or c=2.0 appear optimal because they maintain consistently low regret across all timesteps.
Avoid c=1.5 and c=0.5 at long horizons, as they show slightly higher regret.
UCB significantly outperforms Softmax and Epsilon-Greedy in regret minimization.
It is robust to variations in c, though c=0.1 or c=2.0 may be slightly preferable. Regret does not explode over time, unlike Softmax (τ=0.001, 50.0) or Epsilon-Greedy (ε=0.5, 0.9).


#### Optional: Please explore different values of epsilon, tau and verify how does the behaviour changes.

explore_epgreedy_epsilons =   [0.0001, 0.005, 0.1, 0.2, 0.3, 0.4, 0.5, 0.6,0.7,0.8]
epgreedy_policies = [EpGreedyPolicy(ep, env.arm_ids) for ep in explore_epgreedy_epsilons]

#experimenting with different timesteps , timesteps represents the number of trials the policy has with the environment
print("timestep:5")
plot_reward_curve_and_print_regret(env, epgreedy_policies, timesteps=5, num_runs=500)
print("timestep:10")
plot_reward_curve_and_print_regret(env, epgreedy_policies, timesteps=10, num_runs=500)
print("timestep:20")
plot_reward_curve_and_print_regret(env, epgreedy_policies, timesteps=20, num_runs=500)
print("timestep:50")
plot_reward_curve_and_print_regret(env, epgreedy_policies, timesteps=50, num_runs=500)
print("timestep:100")
plot_reward_curve_and_print_regret(env, epgreedy_policies, timesteps=100, num_runs=500)
print("timestep:200")
plot_reward_curve_and_print_regret(env, epgreedy_policies, timesteps=200, num_runs=500)
print("timestep:500")
plot_reward_curve_and_print_regret(env, epgreedy_policies, timesteps=500, num_runs=500)
print("timestep:1000")
plot_reward_curve_and_print_regret(env, epgreedy_policies, timesteps=1000, num_runs=500)
print("timestep:10000")
plot_reward_curve_and_print_regret(env, epgreedy_policies, timesteps=10000, num_runs=500)

explore_softmax_taus =  [2.0, 3.0, 4.0, 5.0,6.0, 7.0, 10.0]
softmax_polices = [SoftmaxPolicy(tau, env.arm_ids) for tau in explore_softmax_taus]
print("Timestep:5")
plot_reward_curve_and_print_regret(env, softmax_polices, timesteps=50, num_runs=500)
print("Timestep:10")
plot_reward_curve_and_print_regret(env, softmax_polices, timesteps=50, num_runs=500)
print("Timestep:20")
plot_reward_curve_and_print_regret(env, softmax_polices, timesteps=50, num_runs=500)
print("Timestep:50")
plot_reward_curve_and_print_regret(env, softmax_polices, timesteps=50, num_runs=500)
print("Timestep:100")
plot_reward_curve_and_print_regret(env, softmax_polices, timesteps=100, num_runs=500)
print("Timestep:200")
plot_reward_curve_and_print_regret(env, softmax_polices, timesteps=200, num_runs=500)


!pip install nbconvert
!jupyter nbconvert --to pdf MA23M010.ipynb
